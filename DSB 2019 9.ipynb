{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows',None)\npd.set_option('display.max_columns', None)\nimport datetime\nimport catboost\nfrom catboost import CatBoostClassifier,Pool\nimport time\nfrom tqdm import tqdm_notebook as tqdm\nimport os\nimport random\nimport itertools\nimport json\nimport pprint\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import cohen_kappa_score\nfrom collections import Counter\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport lightgbm as lgb\nimport xgboost as xgb\nimport copy\nfrom functools import partial\nimport scipy as sp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spec(value,*args):\n    i= specs[specs['event_id'] == value].index.values[-1]\n    print('Index :',i)\n    print('Event_code :',train[train['event_id'] == value]['event_code'].unique()[-1])\n    for arg in args:\n        if(arg == 'info'):\n         print(specs[arg][i])\n        elif(arg == 'args'):\n         print(pprint.pprint(json.loads(specs[arg][i])))\n        else:\n         print('Nothing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def event(value):\n    i = train[train['event_id'] == value].index.values[-1]\n    print(pprint.pprint(json.loads(train['event_data'][i])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reset(df):\n    df = df.reset_index(drop=False)\n    df.drop(columns=['index'],axis=1,inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_features(df):\n    total_features = []\n    clip_features = ['num_clip_watched']\n    game_features = ['num_unique_games','total_game_actions','total_duration_spend_for_game','total_game_attempts','total_correct_attempts','last_game_played_accuracy','gaming_accuracy']\n    activity_features = []\n    total_data = {}\n    total_features = clip_features + game_features + activity_features\n    for i,install_id in tqdm(df.groupby('installation_id',sort=False)):\n\n        clip_data = {eve : 0 for eve in clip_features}\n        game_data = {eve : 0 for eve in game_features}\n        activity_data = {}\n        event_code_count = {eve : 0 for eve in list_of_event_code}\n\n        for j,session in install_id.groupby('game_session',sort=False):\n\n            session_type = session['type'].iloc[0]\n            session_title = session['title'].iloc[0]\n\n            if((session_type == 'Clip')):\n                clip_data['num_clip_watched'] += 1\n\n            elif(session_type == 'Game'):\n                game_data['num_unique_games'] += 1\n                game_data['total_game_actions'] += session['event_code'].count()\n                game_data['total_duration_spend_for_game'] += int(session['game_time'].iloc[-1]/1000)\n                game_data['total_game_attempts'] += session[session['event_code'] == 4020]['event_code'].count()\n                game_data['total_correct_attempts'] += session[session['event_code'] == 4020]['event_data'].map(lambda x:1 if(str(x).find('\"correct\":true') >= 0) else 0).sum()\n                game_data['gaming_accuracy'] += round((game_data['total_correct_attempts']/game_data['total_game_attempts']),3) if(game_data['total_game_attempts']>0) else 0\n                game_data['last_game_played_accuracy'] = round(game_data['total_correct_attempts'] / game_data['total_game_attempts'],3) if(game_data['total_game_attempts']>0) else 0\n            elif(session_type == 'Activity'):\n                pass\n            elif(session_type == 'Assessment'):\n                #game_preprocessing\n\n\n                #Activity preprocessing\n\n\n                #Clip preprocessing\n\n\n\n                total_data[j] = {}\n                total_data[j].update(clip_data)\n                total_data[j].update(activity_data)\n                total_data[j].update(game_data)\n\n                clip_data = {eve : 0 for eve in clip_features}\n                game_data = {eve : 0 for eve in game_features}\n                activity_data = {}\n    return total_features,total_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_validation_data(df,num_samples,random_state):\n    validation_data = df[(df['type'] == 'Assessment') & (((df['event_code'] == 4100) & (df['title'] != 'Bird Measurer (Assessment)')) | ((df['event_code'] == 4110) & (df['title'] == 'Bird Measurer (Assessment)')))]\n    validation_data.drop_duplicates(subset = 'game_session',keep = 'last',inplace=True)\n    if(isinstance(num_samples,float)):\n            validation_data = validation_data.sample(frac = num_samples,random_state = random_state)\n            print(validation_data.shape)\n    else:\n            validation_data = validation_data.sample(n = num_samples,random_state = random_state)\n            print(validation_data.shape)\n    return validation_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=0):\n  random.seed(seed)\n  os.environ['PYTHONHASHSEED'] = str(seed)\n  np.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Rand(start, end, num): \n    res = [] \n    for j in range(num): \n        res.append(random.randint(start, end)) \n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def qwk(a1, a2):\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n    \n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n    \n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n    return 1 - o / e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_qwk_lgb(y_true, y_pred):\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return 'cappa', qwk(y_true, y_pred), True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_qwk_lgb_regr(y_true, y_pred):\n    y_pred[y_pred <= 1.12232214] = 0\n    y_pred[np.where(np.logical_and(y_pred > 1.12232214, y_pred <= 1.73925866))] = 1\n    y_pred[np.where(np.logical_and(y_pred > 1.73925866, y_pred <= 2.22506454))] = 2\n    y_pred[y_pred > 2.22506454] = 3\n    \n    return 'cappa', qwk(y_true, y_pred), True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LGBWrapper_regr(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMRegressor(**params)\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n        if params['objective'] == 'regression':\n            eval_metric = eval_qwk_lgb_regr\n        else:\n            eval_metric = 'auc'\n            \n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        \n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_metric,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       categorical_feature=categorical_columns)         \n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict(self, X_test):\n        return self.model.predict(X_test, num_iteration=self.model.best_iteration_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RegressorModel(object):\n\n    def __init__(self, columns: list = None, model_wrapper=None):\n        \n        self.columns = columns\n        self.model_wrapper = model_wrapper\n        self.result_dict = {}\n        self.train_one_fold = False\n        self.preprocesser = None\n\n    def fit(self, X: pd.DataFrame, y,\n            X_holdout: pd.DataFrame = None, y_holdout=None,\n            folds=None,\n            params: dict = None,\n            eval_metric='rmse',\n            cols_to_drop: list = None,\n            preprocesser=None,\n            transformers: dict = None,\n            adversarial: bool = False,\n            plot: bool = True):\n\n        if folds is None:\n            folds = KFold(n_splits=3, random_state=42)\n            self.train_one_fold = True\n\n        self.columns = X.columns if self.columns is None else self.columns\n        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n        self.trained_transformers = {k: [] for k in transformers}\n        self.transformers = transformers\n        self.models = []\n        self.folds_dict = {}\n        self.eval_metric = eval_metric\n        n_target = 1\n        self.oof = np.zeros((len(X), n_target))\n        self.n_target = n_target\n\n        X = X[self.columns]\n        if X_holdout is not None:\n            X_holdout = X_holdout[self.columns]\n\n        if preprocesser is not None:\n            self.preprocesser = preprocesser\n            self.preprocesser.fit(X, y)\n            X = self.preprocesser.transform(X, y)\n            self.columns = X.columns.tolist()\n            if X_holdout is not None:\n                X_holdout = self.preprocesser.transform(X_holdout)\n\n        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n\n            if X_holdout is not None:\n                X_hold = X_holdout.copy()\n            else:\n                X_hold = None\n            self.folds_dict[fold_n] = {}\n            if params['verbose']:\n                print(f'Fold {fold_n + 1} started at {time.ctime()}')\n\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            if self.train_one_fold:\n                X_train = X[self.original_columns]\n                y_train = y\n                X_valid = None\n                y_valid = None\n\n            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n\n            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n            if adversarial:\n                X_new1 = X_train.copy()\n                if X_valid is not None:\n                    X_new2 = X_valid.copy()\n                elif X_holdout is not None:\n                    X_new2 = X_holdout.copy()\n                X_new = pd.concat([X_new1, X_new2], axis=0)\n                y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n                X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n                                                                      \n            model = copy.deepcopy(self.model_wrapper)  \n            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)                                   \n                                                                                                                                            \n            self.folds_dict[fold_n]['scores'] = model.best_score_\n            if self.oof.shape[0] != len(X):\n                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n            if not adversarial:\n                self.oof[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n            fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n                                           columns=['feature', 'importance'])\n            self.feature_importances = self.feature_importances.append(fold_importance)\n            self.models.append(model)\n\n        self.feature_importances['importance'] = self.feature_importances['importance'].astype(int)\n\n        self.calc_scores_()\n        if plot:\n            fig, ax = plt.subplots(figsize=(16, 12))\n            plt.subplot(2, 2, 1)\n            self.plot_feature_importance(top_n=20)\n            plt.subplot(2, 2, 2)\n            self.plot_metric()\n            plt.subplot(2, 2, 3)\n            plt.hist(y.values.reshape(-1, 1) - self.oof)\n            plt.title('Distribution of errors')\n            plt.subplot(2, 2, 4)\n            plt.hist(self.oof)\n            plt.title('Distribution of oof predictions');\n\n    def transform_(self, datasets, cols_to_drop):\n        for name, transformer in self.transformers.items():\n            transformer.fit(datasets['X_train'], datasets['y_train'])\n            datasets['X_train'] = transformer.transform(datasets['X_train'])\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n            self.trained_transformers[name].append(transformer)\n        if cols_to_drop is not None:\n            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n\n            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n        self.cols_to_drop = cols_to_drop\n\n        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n\n    def calc_scores_(self):\n        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n        self.scores = {}\n        for d in datasets:\n            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n            self.scores[d] = np.mean(scores)\n\n    def predict(self, X_test, averaging: str = 'usual'):\n        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n        if self.preprocesser is not None:\n            X_test = self.preprocesser.transform(X_test)\n        for i in range(len(self.models)):\n            X_t = X_test.copy()\n            for name, transformers in self.trained_transformers.items():\n                X_t = transformers[i].transform(X_t)\n\n            if self.cols_to_drop is not None:\n                cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n                X_t = X_t.drop(cols_to_drop, axis=1)\n            y_pred = self.models[i].predict(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n\n            # if case transformation changes the number of the rows\n            if full_prediction.shape[0] != len(y_pred):\n                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n\n            if averaging == 'usual':\n                full_prediction += y_pred\n            elif averaging == 'rank':\n                full_prediction += pd.Series(y_pred).rank().values\n\n        return full_prediction / len(self.models)\n\n    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n\n        top_feats = self.get_top_features(drop_null_importance, top_n)\n        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n        feature_importances['feature'] = feature_importances['feature'].astype(str)\n        top_feats = [str(i) for i in top_feats]\n        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n        plt.title('Feature importances')\n\n    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n    \n        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n        if drop_null_importance:\n            grouped_feats = grouped_feats[grouped_feats != 0]\n        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n\n    def plot_metric(self):\n        \n        full_evals_results = pd.DataFrame()\n        for model in self.models:\n            evals_result = pd.DataFrame()\n            for k in model.model.evals_result_.keys():\n                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n            full_evals_results = full_evals_results.append(evals_result)\n\n        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n                                                                                            'variable': 'dataset'})\n        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n        plt.title('Training progress')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MainTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n        self.convert_cyclical = convert_cyclical\n        self.create_interactions = create_interactions\n        self.feats_for_interaction = None\n        self.n_interactions = n_interactions\n\n    def fit(self, X, y=None):\n\n        if self.create_interactions:\n            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n                                          or 'mean' in col or 'max' in col or 'std' in col\n                                          or 'attempt' in col]\n            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        if self.create_interactions:\n            for col1 in self.feats_for_interaction1:\n                for col2 in self.feats_for_interaction2:\n                    data[f'{col1}_int_{col2}'] = data[col1] * data[col2]\n\n        if self.convert_cyclical:\n            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] / 23.0)\n            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] / 23.0)\n            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] / 23.0)\n            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] / 23.0)\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n        self.main_cat_features = main_cat_features\n        self.num_cols = num_cols\n\n    def fit(self, X, y=None):\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n        return -qwk(y, X_p)\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n    def coefficients(self):\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_dict_to_list(d):\n    for i,j in d.items():\n        d[i] = [v for k,v in d[i].items()]\n    return d","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\ntest = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\ntrain = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\nprint(time.time() - start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\nspecs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assessment_id = list(train[train['type'] == 'Assessment']['installation_id'].unique())\ntrain = train.loc[train['installation_id'].isin(assessment_id)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.reset_index(drop=False)\ntrain.drop(columns = ['index'],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_event_code = set(train['event_code'].unique()).union(set(test['event_code'].unique()))\nlist_of_event_code = list(list_of_event_code)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_game = train[(train['type'] == 'Game') | (train['type'] == 'Assessment')]\n#train_game = train_game.reset_index(drop=False)\n#train_game.drop(columns = ['index'],axis = 1, inplace = True)\n\n#train_activity = train[(train['type'] == 'Assessment') | (train['type'] == 'Activity')]\n#train_activity = train_activity.reset_index(drop=False)\n#train_activity.drop(columns = ['index'],axis = 1, inplace = True)\n\n#train_clip = train[(train['type'] == 'Assessment') | (train['type'] == 'Clip')]\n#train_clip = train_clip.reset_index(drop=False)\n#train_clip.drop(columns = ['index'],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_mapper = {'index': 'game_session',0:'num_clip_watched',1:'num_unique_games',2:'total_game_actions',3:'total_duration_spend_for_game',4:'total_game_attempts',5:'total_correct_attempts',6:'last_game_played_accuracy',7:'gaming_accuracy'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train**"},{"metadata":{"trusted":true},"cell_type":"code","source":"features, train_dict = get_features(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in tqdm(train_dict.items()):\n    train_dict[i] = [v for k,v in train_dict[i].items()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[(train['type'] == 'Assessment')].drop_duplicates(subset='game_session',keep = 'first').reset_index(drop=False).drop(columns=['index'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge = pd.DataFrame().from_dict(train_dict)\ntrain_merge = train_merge.T.reset_index(drop=False).rename(columns = column_mapper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(train_merge,on='game_session',how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"features, test_dict = get_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in tqdm(test_dict.items()):\n    test_dict[i] = [v for k,v in test_dict[i].items()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test[(test['type'] == 'Assessment')].drop_duplicates(subset='game_session',keep = 'first').reset_index(drop=False).drop(columns=['index'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_merge = pd.DataFrame().from_dict(test_dict)\ntest_merge = test_merge.T.reset_index(drop=False).rename(columns = column_mapper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.merge(test_merge,on='game_session',how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['game_session'].nunique())\nprint(train_labels['game_session'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.loc[train['game_session'].isin(train_labels['game_session'].unique().tolist())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_mapper = dict(zip(train_labels['game_session'],train_labels['accuracy_group']))\ntrain['accuracy_group'] = train['game_session'].map(accuracy_mapper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_user_activities = list(set(train['title'].value_counts().index).union(set(test['title'].value_counts().index)))\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n\ntrain['title'] = train['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\ntrain_labels['title'] = train_labels['title'].map(activities_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world = list(set(train['world'].unique().tolist()).union(set(test['world'].unique().tolist())))\nworld = dict(zip(world,np.arange(len(world))))\n\ntrain['world'] = train['world'].map(world)\ntest['world'] = test['world'].map(world)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#instal_id_mapper = list(set(train['installation_id'].unique().tolist()).union(set(test['installation_id'].unique().tolist())))\n#instal_id_mapper = dict(zip(instal_id_mapper,np.arange(len(instal_id_mapper))))\n\n#for df in tqdm([train,test]):\n#    df['installation_id'] = df['installation_id'].map(instal_id_mapper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['game_session', 'installation_id', 'timestamp', 'accuracy_group','event_id','event_data','event_count','event_code','game_time','type','total_correct_attempts','title']\nn_fold = 5\nfolds = GroupKFold(n_splits=n_fold)\ny = train['accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = ['title','world','num_unique_games']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {  'n_estimators':2000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n             'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.04,\n            'feature_fraction': 0.9,\n             'max_depth': 15,\n            'lambda_l1': 0.87,  \n            'lambda_l2': 0.96,\n            'verbose': 100,\n            'early_stopping_rounds': 100, \n            'eval_metric': 'cappa'\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(48)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mt = MainTransformer()\nft = FeatureTransformer()\ntransformers = {'ft': ft}\nregressor_model1 = RegressorModel(model_wrapper=LGBWrapper_regr())\nregressor_model1.fit(X=train, y=y, folds=folds, params=params, preprocesser=mt, transformers=transformers,\n                    eval_metric='cappa', cols_to_drop=cols_to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pr1 = regressor_model1.predict(train)\n\noptR = OptimizedRounder()\noptR.fit(pr1.reshape(-1,), y)\ncoefficients = optR.coefficients()\ncoefficients = list(coefficients)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefficients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt_preds = optR.predict(pr1.reshape(-1, ), coefficients)\nqwk(y, opt_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pr1 = regressor_model1.predict(test)\npr1[pr1 <= coefficients[0]] = 0\npr1[np.where(np.logical_and(pr1 > coefficients[0], pr1 <= coefficients[1]))] = 1\npr1[np.where(np.logical_and(pr1 > coefficients[1], pr1 <= coefficients[2]))] = 2\npr1[pr1 > coefficients[2]] = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['accuracy_group'] = pr1\nnew_test = test.drop_duplicates(subset='installation_id',keep='last').reset_index(drop=False).drop(columns=['index'],axis=1)\ntest_prediction_dict = dict(zip(new_test['installation_id'],new_test['accuracy_group']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['accuracy_group'] = submission['installation_id'].map(test_prediction_dict)\nsubmission['accuracy_group'] = submission['accuracy_group'].astype(int)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Garbage**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def new():\n    cols = train.columns.tolist()\n    cols.remove('installation_id')\n    for i ,data in train.groupby('installation_id',sort=False):\n        new_data = data[cols]\n        new_data['installation_id'] = i\n        break\n    return new_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_features(df:pd.DataFrame(),which):\n    \n    new_df_list_values = []\n    event_code_count_dict = {eve : 0 for eve in list_of_event_code}\n    new_df_dict = {}\n        \n    df = df.reset_index(drop= False)\n    df.drop(columns=['index'],axis=1,inplace=True)\n    \n    for i,data in tqdm(df.groupby('installation_id',sort=False)):\n        print(i)\n        new_data = data[df.columns.tolist()]\n        new_data['installation_id'] = i\n        \n        new_data = new_data.reset_index(drop= False)\n        new_data.drop(columns=['index'],axis=1,inplace=True)\n        \n        a1 = new_data[new_data['type'] == 'Assessment'].drop_duplicates(subset = 'game_session',keep = 'first').index.tolist()\n        a2 = new_data[new_data['type'] == 'Assessment'].drop_duplicates(subset = 'game_session',keep = 'last').index.tolist()\n\n        a = []\n        a.append(0)\n        print(a2)\n        for i in range(len(a2)):\n            a.append(a2[i])\n            a.append(a2[i]+1)\n        a.pop(len(a)-1)\n        print(a)\n\n        for i in range(0,len(a),2):\n            print(a[i] , 'to' , a[i+1])\n            new_df = data.iloc[a[i]:a[i+1]+1,:]\n            g_session = new_df[new_df['type'] == 'Assessment']['game_session'].unique()[-1]\n            print(g_session)\n            new_df_dict[g_session] = {}\n            new_df_dict[g_session]['num_game_session'] = new_df['game_session'].nunique()\n            new_df_dict[g_session]['num_event_id'] = new_df['event_id'].nunique()\n            new_df_dict[g_session]['num_actions_before_assessment'] = len(new_df['event_code'])\n            new_df_dict[g_session][f'num_unique_{which}'] = new_df[new_df['type'] == which]['title'].nunique()\n            if(which == 'Game'):\n                new_df_dict[g_session]['game_attempts'] = new_df[new_df['event_code'] == 4020]['event_code'].count()\n                new_df_dict[g_session]['mean_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].mean()\n                new_df_dict[g_session]['max_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].max()\n                new_df_dict[g_session]['min_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].min()\n                new_df_dict[g_session]['std_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].std()\n                new_df['gaming_accuracy'] = new_df[(new_df['type'] == 'Game') & (new_df['event_code'] == 4020)]['event_data'].map(lambda x:1 if(str(x).find('\"correct\":true')) else 0)\n                new_df['gaming_accuracy'].fillna(0,inplace=True)\n                new_df_dict[g_session]['gaming_accuracy'] = new_df['gaming_accuracy'].sum()/new_df_dict[g_session]['game_attempts']\n            elif(which == 'Activity'):\n                new_df_dict[g_session]['Activity actions'] = new_df[new_df['type'] == 'Activity']['event_id'].count()\n\n            for k,v in event_code_count_dict.items():\n                new_df_dict[g_session][k] = v\n\n            for i in event_code_count_dict.keys():\n                 new_df_dict[g_session][i] = new_df[new_df['event_code'] == i]['event_code'].count()\n            #break\n    return new_df_dict      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_features(df:pd.DataFrame(),which):\n    \n    new_df_list_values = []\n    event_code_count_dict = {}\n    new_df_dict = {}\n    \n    for i in train['event_code'].unique().tolist():\n        event_code_count_dict[i] = 0\n        \n    df = df.reset_index(drop= False)\n    df.drop(columns=['index'],axis=1,inplace=True)\n    \n    a1 = df[df['type'] == 'Assessment'].drop_duplicates(subset = 'game_session',keep = 'first').index.tolist()\n    a2 = df[df['type'] == 'Assessment'].drop_duplicates(subset = 'game_session',keep = 'last').index.tolist()\n    \n    a = []\n    a.append(0)\n    for i in range(len(a2)):\n        a.append(a2[i])\n        a.append(a2[i]+1)\n    a.pop(len(a)-1)\n    \n    for i in tqdm(range(len(a))):\n        new_df = df.iloc[a[i]:a[i+1],:]\n        ids  = new_df['installation_id'].unique().tolist()[-1]\n        new_df_dict[ids] = {}\n        new_df_dict['num_game_session'] = new_df['game_session'].nunique()\n        new_df_dict['num_event_id'] = new_df['event_id'].nunique()\n        new_df_dict['num_actions_before_assessment'] = len(new_df['event_code'])\n        new_df_dict[f'num_unique_{which}'] = new_df[new_df['type'] == which]['title'].nunique()\n        if(which == 'Game'):\n            new_df_dict['game_attempts'] = new_df[new_df['event_code'] == 4020]['event_code'].count()\n            new_df_dict['mean_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].mean()\n            new_df_dict['max_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].max()\n            new_df_dict['min_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].min()\n            new_df_dict['std_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].std()\n            new_df['gaming_accuracy'] = new_df[(new_df['type'] == 'Game') & (new_df['event_code'] == 4020)]['event_data'].map(lambda x:1 if(str(x).find('\"correct\":true')) else 0)\n            new_df['gaming_accuracy'].fillna(0,inplace=True)\n            new_df_dict['gaming_accuracy'] = new_df['gaming_accuracy'].sum()/new_df_dict['game_attempts']\n            #new_df['time_gap_before_assessment'] = new_df['']\n        elif(which == 'Activity'):\n            new_df_dict['Activity actions'] = new_df[new_df['type'] == 'Activity']['event_id'].count()\n        \n        event_code_count_dict.update(new_df_dict)\n        break\n    print(event_code_count_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_features(df:pd.DataFrame(),which):\n    \n    new_df_list_values = []\n    event_code_count_dict = {eve : 0 for eve in list_of_event_code}\n    new_df_dict = {}\n        \n    df = df.reset_index(drop= False)\n    df.drop(columns=['index'],axis=1,inplace=True)\n    \n    a1 = df[df['type'] == 'Assessment'].drop_duplicates(subset = 'game_session',keep = 'first').index.tolist()\n    a2 = df[df['type'] == 'Assessment'].drop_duplicates(subset = 'game_session',keep = 'last').index.tolist()\n    \n    a = []\n    a.append(0)\n    for i in range(len(a2)):\n        a.append(a2[i])\n        a.append(a2[i]+1)\n    a.pop(len(a)-1)\n    \n    for i in tqdm(range(10)):\n        new_df = df.iloc[a[i]:a[i+1],:]\n        ids  = new_df['installation_id'].unique().tolist()[-1]\n        g_session = new_df[new_df['type'] == 'Assessment']['game_session'].unique()[-1]\n        print(ids,g_session)\n        new_df_dict[ids] = {}\n        new_df_dict[ids][g_session] = {}\n        new_df_dict[ids][g_session]['num_game_session'] = new_df['game_session'].nunique()\n        new_df_dict[ids][g_session]['num_event_id'] = new_df['event_id'].nunique()\n        new_df_dict[ids][g_session]['num_actions_before_assessment'] = len(new_df['event_code'])\n        new_df_dict[ids][g_session][f'num_unique_{which}'] = new_df[new_df['type'] == which]['title'].nunique()\n        if(which == 'Game'):\n            new_df_dict[ids][g_session]['game_attempts'] = new_df[new_df['event_code'] == 4020]['event_code'].count()\n            new_df_dict[ids][g_session]['mean_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].mean()\n            new_df_dict[ids][g_session]['max_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].max()\n            new_df_dict[ids][g_session]['min_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].min()\n            new_df_dict[ids][g_session]['std_game_time'] = new_df[new_df['type'] == 'Game']['game_time'].std()\n            new_df['gaming_accuracy'] = new_df[(new_df['type'] == 'Game') & (new_df['event_code'] == 4020)]['event_data'].map(lambda x:1 if(str(x).find('\"correct\":true')) else 0)\n            new_df['gaming_accuracy'].fillna(0,inplace=True)\n            new_df_dict[ids][g_session]['gaming_accuracy'] = new_df['gaming_accuracy'].sum()/new_df_dict[ids][g_session]['game_attempts']\n        elif(which == 'Activity'):\n            new_df_dict[ids][g_session]['Activity actions'] = new_df[new_df['type'] == 'Activity']['event_id'].count()\n        \n        for k,v in event_code_count_dict.items():\n            new_df_dict[ids][g_session][k] = v\n        \n        for i in event_code_count_dict.keys():\n             new_df_dict[ids][g_session][i] = new_df[new_df['event_code'] == i]['event_code'].count()\n        break\n    return new_df_dict  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def g5():\n    train_column_values = {}\n    for i in tqdm(train['installation_id'].unique().tolist()):\n        df = train[(train['installation_id'] == i) & ((train['type'] == 'Game') | (train['type'] == 'Assessment'))]\n        column_values_instance = get_features(df,'Game')\n        train_column_values.update(column_values_instance)\n    train_column_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def g4():\n    per_install_id_features = {}\n    event_code_count_per_id = {}\n    for i,data in tqdm(train.groupby('installation_id',sort = False)):\n        new_data = data[data.columns.tolist()]\n        per_install_id_features[i] = {}\n        for j in ['Game','Activity','Clip']:\n            per_install_id_features[i][f'num_{j}'] = new_data[new_data['type'] == j]['title'].nunique()\n        per_install_id_features[i]['total_num_actions'] = new_data[(new_data['type'] == 'Game') | (new_data['type'] == 'Activity') | (new_data['type'] == 'Clip')]['event_id'].count()\n        per_install_id_features[i]['total_game_attempts'] = new_data[(new_data['type'] == 'Game') & (new_data['event_code'] == 4020)]['event_code'].count()\n        new_data['correct_attempts'] = new_data[(new_data['type'] == 'Game') & (new_data['event_code'] == 4020)]['event_data'].map(lambda x:1 if(str(x).find('\"correct\":true'))>=0 else 0)\n        per_install_id_features[i]['total_correct_game_attempts'] = new_data['correct_attempts'].sum()\n        per_install_id_features[i]['total_accuracy'] = per_install_id_features[i]['total_correct_game_attempts']/per_install_id_features[i]['total_game_attempts'] if per_install_id_features[i]['total_game_attempts'] >0 else 0\n\n    per_install_id_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def g3():\n    clip_features = ['num_clip_watched']\n    game_features = ['num_unique_games','total_game_actions','total_duration_spend_for_game','total_game_attempts','total_correct_attempts','last_game_played_accuracy','gaming_accuracy']\n    activity_features = []\n    total_data = {}\n\n    for i,install_id in tqdm(test.groupby('installation_id',sort=False)):\n\n        clip_data = {eve : 0 for eve in clip_features}\n        game_data = {eve : 0 for eve in game_features}\n        activity_data = {}\n        event_code_count = {eve : 0 for eve in list_of_event_code}\n\n        for j,session in install_id.groupby('game_session',sort=False):\n\n            session_type = session['type'].iloc[0]\n            session_title = session['title'].iloc[0]\n\n            if((session_type == 'Clip')):\n                clip_data['num_clip_watched'] += 1\n\n            elif(session_type == 'Game'):\n                game_data['num_unique_games'] += 1\n                game_data['total_game_actions'] += session['event_code'].count()\n                game_data['total_duration_spend_for_game'] += int(session['game_time'].iloc[-1]/1000)\n                game_data['total_game_attempts'] += session[session['event_code'] == 4020]['event_code'].count()\n                game_data['total_correct_attempts'] += session[session['event_code'] == 4020]['event_data'].map(lambda x:1 if(str(x).find('\"correct\":true') >= 0) else 0).sum()\n                game_data['gaming_accuracy'] += np.round((game_data['total_correct_attempts']/game_data['total_game_attempts']),3)\n                game_data['last_game_played_accuracy'] = np.round(game_data['total_correct_attempts'] / game_data['total_game_attempts'],3) if(game_data['total_game_attempts']>0) else 0\n            elif(session_type == 'Activity'):\n                pass\n            elif(session_type == 'Assessment'):\n                #game_preprocessing\n\n\n                #Activity preprocessing\n\n\n                #Clip preprocessing\n\n\n\n                total_data[j] = {}\n                total_data[j].update(clip_data)\n                total_data[j].update(activity_data)\n                total_data[j].update(game_data)\n\n                clip_data = {eve : 0 for eve in clip_features}\n                game_data = {eve : 0 for eve in game_features}\n                activity_data = {}\n        #break   \n\n    total_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def g2():\n    train = train[(train['type'] == 'Assessment') & (((train['event_code'] == 4100) & (train['title'] != 'Bird Measurer (Assessment)')) | ((train['event_code'] == 4110) & (train['title'] == 'Bird Measurer (Assessment)')))]\n\n    session_count = train['game_session'].value_counts().to_dict()\n    train['assessment_attempt_count'] = train['game_session'].map(session_count)\n\n    train['contains_true_assessment'] = train['event_data'].map(lambda x: True if (x.find('\"correct\":true')>=0) else False)\n\n    change_value = {\n        True : 1,\n        False : 0\n    }\n    train['contains_true_assessment'] = train['contains_true_assessment'].map(change_value)\n\n    correct_attempt = dict(train.groupby('game_session',sort=False)['contains_true_assessment'].sum())\n    train['contains_true_assessment_count'] = train['game_session'].map(correct_attempt)\n\n    for c in ['contains_true_assessment']:\n        train.pop(c)\n\n    train['accumulated_accuracy'] = np.where((train['contains_true_assessment_count'] == 0),0,(train['contains_true_assessment_count']/train['assessment_attempt_count']))\n\n    train.loc[(train['type'] == 'Assessment'), 'accuracy_group'] = 0\n    train.loc[(train['accumulated_accuracy'] == 1) & (train['type'] == 'Assessment'), 'accuracy_group'] = 3\n    train.loc[(train['accumulated_accuracy'] == 0.5) & (train['type'] == 'Assessment'), 'accuracy_group'] = 2\n    train.loc[(train['accumulated_accuracy'] < 0.5) & (train['accumulated_accuracy'] > 0) & (train['assessment_attempt_count'] > 0) & (train['type'] == 'Assessment'), 'accuracy_group'] = 1\n\n    train.rename(columns = {'contains_true_assessment_count': 'num_correct',\n                            'accumulated_accuracy':'accuracy',\n                            'assessment_attempt_count': 'total_attempt'},inplace=True)\n    train = train.drop_duplicates(subset = 'game_session',keep = 'last')\n    train = train.reset_index(drop=False)\n    train.drop(columns = ['index'],axis = 1, inplace = True)\n    train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def g1():\n    test =  test[(test['type'] == 'Assessment') &  ((test['event_count'] == 1) | ((test['event_code'] == 4100) & (test['title'] != 'Bird Measurer (Assessment)')) | ((test['event_code'] == 4110) & (test['title'] == 'Bird Measurer (Assessment)')))]\n    session_count = test['game_session'].value_counts().to_dict()\n    test['assessment_attempt_count'] = test['game_session'].map(session_count)\n\n    test['contains_true_assessment'] = test['event_data'].map(lambda x: True if (x.find('\"correct\":true')>=0) else False)\n\n    change_value = {\n        True : 1,\n        False : 0\n    }\n    test['contains_true_assessment'] = test['contains_true_assessment'].map(change_value)\n\n    correct_attempt = dict(test.groupby('game_session',sort=False)['contains_true_assessment'].sum())\n    test['contains_true_assessment_count'] = test['game_session'].map(correct_attempt)\n\n\n    for c in ['contains_true_assessment']:\n        test.pop(c)\n\n    test['accumulated_accuracy'] = np.where((test['contains_true_assessment_count'] == 0),0,(test['contains_true_assessment_count']/test['assessment_attempt_count']))\n\n    test.loc[(test['type'] == 'Assessment'), 'accuracy_group'] = 0\n    test.loc[(test['accumulated_accuracy'] == 1) & (test['type'] == 'Assessment'), 'accuracy_group'] = 3\n    test.loc[(test['accumulated_accuracy'] == 0.5) & (test['type'] == 'Assessment'), 'accuracy_group'] = 2\n    test.loc[(test['accumulated_accuracy'] < 0.5) & (test['accumulated_accuracy'] > 0) & (test['assessment_attempt_count'] > 0) & (test['type'] == 'Assessment'), 'accuracy_group'] = 1\n\n    test.rename(columns = {'contains_true_assessment_count': 'num_correct',\n                            'accumulated_accuracy':'accuracy',\n                            'assessment_attempt_count': 'total_attempt'},inplace=True)\n\n    test = test.drop_duplicates(subset = 'game_session',keep = 'last')\n    test = test.reset_index(drop=False)\n    test.drop(columns = ['index'],axis = 1, inplace = True)\n    test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def g8():\n    oof = np.zeros(len(x))\n    NFOLDS = 5\n    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=2019)\n\n\n    for fold, (trn_idx, test_idx) in enumerate(folds.split(x, y)):\n\n        print(f'Training on fold {fold+1}')\n        clf = make_classifier()\n        clf.fit(x.loc[trn_idx], y.loc[trn_idx], eval_set=(x.loc[test_idx], y.loc[test_idx]),\n                              use_best_model=True, verbose=500)\n\n        oof[test_idx] = clf.predict(x.loc[test_idx]).reshape(len(test_idx))\n        print('OOF QWK:', qwk(y, oof))\n\n    print('-' * 30)\n    print('OOF QWK:', qwk(y, oof))\n    print('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n\n        top_feats = self.get_top_features(drop_null_importance, top_n)\n        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n        feature_importances['feature'] = feature_importances['feature'].astype(str)\n        top_feats = [str(i) for i in top_feats]\n        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n        plt.title('Feature importances')\n\n    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n    \n        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n        if drop_null_importance:\n            grouped_feats = grouped_feats[grouped_feats != 0]\n        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n\n    def plot_metric(self):\n        \n        full_evals_results = pd.DataFrame()\n        for model in self.models:\n            evals_result = pd.DataFrame()\n            for k in model.model.evals_result_.keys():\n                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n            full_evals_results = full_evals_results.append(evals_result)\n\n        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n                                                                                            'variable': 'dataset'})\n        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n        plt.title('Training progress')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}